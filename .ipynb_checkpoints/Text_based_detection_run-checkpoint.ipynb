{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from chart_studio import plotly as py\n",
    "#import plotly.figure_factory as ff\n",
    "from scipy import stats\n",
    "\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Activation, GlobalAveragePooling1D, Flatten, Concatenate, Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "from keras.preprocessing.text import one_hot, text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import fnmatch\n",
    "\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from ast import literal_eval\n",
    "import statistics\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOWS_SIZE = 10\n",
    "labels=['none','mild','moderate','moderately severe', 'severe']\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/prahlad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/prahlad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/media/prahlad/New Volume/DIAC-WOZ/transcripts/\"\n",
    "#transcripts_to_dataframe(data_path) \n",
    "all_participants = pd.read_csv(data_path + 'all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants.columns =  ['index','personId', 'question', 'answer']\n",
    "all_participants = all_participants.astype({\"index\": int, \"personId\": float, \"question\": str, \"answer\": str })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_mix = all_participants.copy()\n",
    "all_participants_mix['answer'] = all_participants_mix.apply(lambda row: text_to_wordlist(row.answer).split(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_mix_stopwords = all_participants.copy()\n",
    "all_participants_mix_stopwords['answer'] = all_participants_mix_stopwords.apply(lambda row: text_to_wordlist(row.answer, remove_stopwords=False).split(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7373\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in all_participants_mix['answer'].tolist()]\n",
    "words = set(itertools.chain(*words))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stop = [w for w in all_participants_mix_stopwords['answer'].tolist()]\n",
    "words_stop = set(itertools.chain(*words_stop))\n",
    "vocab_size_stop = len(words_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(all_participants_mix['answer'])\n",
    "tokenizer.fit_on_sequences(all_participants_mix['answer'])\n",
    "\n",
    "all_participants_mix['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/media/prahlad/New Volume/DIAC-WOZ/model_glove_lstm_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):    \n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    else:\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    \n",
    "    text = re.sub(r\"\\<\", \" \", text)\n",
    "    text = re.sub(r\"\\>\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(text, model):\n",
    "    word_list = text_to_wordlist(text)\n",
    "    list_of_words = word_list.split(\" \")\n",
    "    sequences = tokenizer.texts_to_sequences([word_list])\n",
    "    word_tokens = sequences[0]\n",
    "    size = len(word_tokens)\n",
    "    test_phrases = []\n",
    "    for i in range(size):\n",
    "        tokens = word_tokens[i:min(i+windows_size,size)]  \n",
    "        test_phrases.append(tokens)\n",
    "    sequences_input = test_phrases\n",
    "    sequences_input =  pad_sequences(sequences_input, value=0, padding=\"post\", maxlen=windows_size)\n",
    "    \n",
    "    predicted_classes = []\n",
    "    for sequence in sequences_input:\n",
    "        input_a = np.asarray([sequence])\n",
    "        pred = model.predict(input_a, batch_size=None, verbose=0, steps=None)\n",
    "        predicted_classes.append(np.argmax(pred))\n",
    "        \n",
    "    predicted_class = statistics.mode(predicted_classes)\n",
    "    print(labels[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    }
   ],
   "source": [
    "sen = \"To be honest, I find myself depressed most of the time. I think I really need some help!\"\n",
    "test_model(sen, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/media/prahlad/New Volume/DIAC-WOZ/transcripts/\"\n",
    "#transcripts_to_dataframe(data_path) \n",
    "all_participants = pd.read_csv(data_path + 'all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_308 = all_participants[all_participants.personId==308].answer.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"\"\n",
    "for answer in answers_308:\n",
    "    if type(answer)==str or not math.isnan(answer):\n",
    "        sen+=answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe\n"
     ]
    }
   ],
   "source": [
    "test_model(sen, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

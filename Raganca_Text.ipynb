{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from chart_studio import plotly as py\n",
    "#import plotly.figure_factory as ff\n",
    "from scipy import stats\n",
    "\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Activation, GlobalAveragePooling1D, Flatten, Concatenate, Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "from keras.preprocessing.text import one_hot, text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import fnmatch\n",
    "\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from ast import literal_eval\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WINDOWS_SIZE = 10\n",
    "labels=['none','mild','moderate','moderately severe', 'severe']\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcripts_to_dataframe(directory):\n",
    "    rows_list = []\n",
    "        \n",
    "    filenames = os.listdir(directory)\n",
    "    \n",
    "    if \".DS_Store\" in filenames:\n",
    "        filenames.remove(\".DS_Store\")\n",
    "        \n",
    "    for filename in filenames:\n",
    "        \n",
    "        transcript_path = os.path.join(directory, filename)\n",
    "        transcript = pd.read_csv(transcript_path, sep='\\t')\n",
    "        m = re.search(\"(\\d{3})_TRANSCRIPT.csv\", filename)\n",
    "        if m:\n",
    "            print(filename)\n",
    "            person_id = m.group(1)\n",
    "            p = {}\n",
    "            question = \"\"\n",
    "            answer = \"\"\n",
    "            lines = len(transcript)\n",
    "            for i in range(0, lines):\n",
    "                row = transcript.iloc[i]\n",
    "                if (row[\"speaker\"] == \"Ellie\") or (i == lines - 1):\n",
    "                    p[\"personId\"] = person_id\n",
    "                    if \"(\" in str(question):\n",
    "                        question = question[question.index(\"(\") + 1:question.index(\")\")]\n",
    "                    p[\"question\"] = question\n",
    "                    p[\"answer\"] = answer\n",
    "                    if question != \"\":\n",
    "                        rows_list.append(p)\n",
    "                    p = {}\n",
    "                    answer = \"\"\n",
    "                    question = row[\"value\"]\n",
    "                else:\n",
    "                    answer = str(answer) + \" \" + str(row[\"value\"])\n",
    "\n",
    "    all_participants = pd.DataFrame(rows_list, columns=['personId', 'question', 'answer'])\n",
    "    all_participants.to_csv(directory + 'all.csv', sep=',')\n",
    "    print(\"File was created\")\n",
    "    return all_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458_TRANSCRIPT.csv\n",
      "472_TRANSCRIPT.csv\n",
      "388_TRANSCRIPT.csv\n",
      "402_TRANSCRIPT.csv\n",
      "351_TRANSCRIPT.csv\n",
      "405_TRANSCRIPT.csv\n",
      "465_TRANSCRIPT.csv\n",
      "318_TRANSCRIPT.csv\n",
      "313_TRANSCRIPT.csv\n",
      "345_TRANSCRIPT.csv\n",
      "448_TRANSCRIPT.csv\n",
      "327_TRANSCRIPT.csv\n",
      "456_TRANSCRIPT.csv\n",
      "337_TRANSCRIPT.csv\n",
      "396_TRANSCRIPT.csv\n",
      "412_TRANSCRIPT.csv\n",
      "300_TRANSCRIPT.csv\n",
      "387_TRANSCRIPT.csv\n",
      "349_TRANSCRIPT.csv\n",
      "330_TRANSCRIPT.csv\n",
      "363_TRANSCRIPT.csv\n",
      "302_TRANSCRIPT.csv\n",
      "359_TRANSCRIPT.csv\n",
      "382_TRANSCRIPT.csv\n",
      "442_TRANSCRIPT.csv\n",
      "389_TRANSCRIPT.csv\n",
      "344_TRANSCRIPT.csv\n",
      "417_TRANSCRIPT.csv\n",
      "484_TRANSCRIPT.csv\n",
      "401_TRANSCRIPT.csv\n",
      "311_TRANSCRIPT.csv\n",
      "409_TRANSCRIPT.csv\n",
      "315_TRANSCRIPT.csv\n",
      "471_TRANSCRIPT.csv\n",
      "447_TRANSCRIPT.csv\n",
      "461_TRANSCRIPT.csv\n",
      "374_TRANSCRIPT.csv\n",
      "303_TRANSCRIPT.csv\n",
      "469_TRANSCRIPT.csv\n",
      "489_TRANSCRIPT.csv\n",
      "320_TRANSCRIPT.csv\n",
      "352_TRANSCRIPT.csv\n",
      "367_TRANSCRIPT.csv\n",
      "425_TRANSCRIPT.csv\n",
      "371_TRANSCRIPT.csv\n",
      "380_TRANSCRIPT.csv\n",
      "321_TRANSCRIPT.csv\n",
      "455_TRANSCRIPT.csv\n",
      "454_TRANSCRIPT.csv\n",
      "373_TRANSCRIPT.csv\n",
      "370_TRANSCRIPT.csv\n",
      "479_TRANSCRIPT.csv\n",
      "481_TRANSCRIPT.csv\n",
      "375_TRANSCRIPT.csv\n",
      "449_TRANSCRIPT.csv\n",
      "424_TRANSCRIPT.csv\n",
      "433_TRANSCRIPT.csv\n",
      "406_TRANSCRIPT.csv\n",
      "446_TRANSCRIPT.csv\n",
      "435_TRANSCRIPT.csv\n",
      "334_TRANSCRIPT.csv\n",
      "423_TRANSCRIPT.csv\n",
      "421_TRANSCRIPT.csv\n",
      "411_TRANSCRIPT.csv\n",
      "420_TRANSCRIPT.csv\n",
      "333_TRANSCRIPT.csv\n",
      "385_TRANSCRIPT.csv\n",
      "332_TRANSCRIPT.csv\n",
      "336_TRANSCRIPT.csv\n",
      "443_TRANSCRIPT.csv\n",
      "483_TRANSCRIPT.csv\n",
      "430_TRANSCRIPT.csv\n",
      "312_TRANSCRIPT.csv\n",
      "341_TRANSCRIPT.csv\n",
      "308_TRANSCRIPT.csv\n",
      "356_TRANSCRIPT.csv\n",
      "391_TRANSCRIPT.csv\n",
      "445_TRANSCRIPT.csv\n",
      "379_TRANSCRIPT.csv\n",
      "397_TRANSCRIPT.csv\n",
      "350_TRANSCRIPT.csv\n",
      "400_TRANSCRIPT.csv\n",
      "486_TRANSCRIPT.csv\n",
      "335_TRANSCRIPT.csv\n",
      "340_TRANSCRIPT.csv\n",
      "451_TRANSCRIPT.csv\n",
      "463_TRANSCRIPT.csv\n",
      "473_TRANSCRIPT.csv\n",
      "365_TRANSCRIPT.csv\n",
      "383_TRANSCRIPT.csv\n",
      "407_TRANSCRIPT.csv\n",
      "339_TRANSCRIPT.csv\n",
      "434_TRANSCRIPT.csv\n",
      "353_TRANSCRIPT.csv\n",
      "377_TRANSCRIPT.csv\n",
      "427_TRANSCRIPT.csv\n",
      "314_TRANSCRIPT.csv\n",
      "474_TRANSCRIPT.csv\n",
      "444_TRANSCRIPT.csv\n",
      "354_TRANSCRIPT.csv\n",
      "384_TRANSCRIPT.csv\n",
      "392_TRANSCRIPT.csv\n",
      "366_TRANSCRIPT.csv\n",
      "491_TRANSCRIPT.csv\n",
      "457_TRANSCRIPT.csv\n",
      "459_TRANSCRIPT.csv\n",
      "482_TRANSCRIPT.csv\n",
      "324_TRANSCRIPT.csv\n",
      "490_TRANSCRIPT.csv\n",
      "368_TRANSCRIPT.csv\n",
      "323_TRANSCRIPT.csv\n",
      "426_TRANSCRIPT.csv\n",
      "428_TRANSCRIPT.csv\n",
      "403_TRANSCRIPT.csv\n",
      "399_TRANSCRIPT.csv\n",
      "419_TRANSCRIPT.csv\n",
      "470_TRANSCRIPT.csv\n",
      "467_TRANSCRIPT.csv\n",
      "476_TRANSCRIPT.csv\n",
      "395_TRANSCRIPT.csv\n",
      "306_TRANSCRIPT.csv\n",
      "357_TRANSCRIPT.csv\n",
      "393_TRANSCRIPT.csv\n",
      "322_TRANSCRIPT.csv\n",
      "441_TRANSCRIPT.csv\n",
      "328_TRANSCRIPT.csv\n",
      "329_TRANSCRIPT.csv\n",
      "355_TRANSCRIPT.csv\n",
      "422_TRANSCRIPT.csv\n",
      "462_TRANSCRIPT.csv\n",
      "464_TRANSCRIPT.csv\n",
      "404_TRANSCRIPT.csv\n",
      "372_TRANSCRIPT.csv\n",
      "317_TRANSCRIPT.csv\n",
      "439_TRANSCRIPT.csv\n",
      "343_TRANSCRIPT.csv\n",
      "436_TRANSCRIPT.csv\n",
      "346_TRANSCRIPT.csv\n",
      "331_TRANSCRIPT.csv\n",
      "466_TRANSCRIPT.csv\n",
      "325_TRANSCRIPT.csv\n",
      "431_TRANSCRIPT.csv\n",
      "386_TRANSCRIPT.csv\n",
      "475_TRANSCRIPT.csv\n",
      "378_TRANSCRIPT.csv\n",
      "369_TRANSCRIPT.csv\n",
      "316_TRANSCRIPT.csv\n",
      "492_TRANSCRIPT.csv\n",
      "326_TRANSCRIPT.csv\n",
      "480_TRANSCRIPT.csv\n",
      "487_TRANSCRIPT.csv\n",
      "310_TRANSCRIPT.csv\n",
      "361_TRANSCRIPT.csv\n",
      "381_TRANSCRIPT.csv\n",
      "413_TRANSCRIPT.csv\n",
      "338_TRANSCRIPT.csv\n",
      "414_TRANSCRIPT.csv\n",
      "477_TRANSCRIPT.csv\n",
      "376_TRANSCRIPT.csv\n",
      "429_TRANSCRIPT.csv\n",
      "488_TRANSCRIPT.csv\n",
      "468_TRANSCRIPT.csv\n",
      "360_TRANSCRIPT.csv\n",
      "452_TRANSCRIPT.csv\n",
      "438_TRANSCRIPT.csv\n",
      "410_TRANSCRIPT.csv\n",
      "415_TRANSCRIPT.csv\n",
      "390_TRANSCRIPT.csv\n",
      "408_TRANSCRIPT.csv\n",
      "485_TRANSCRIPT.csv\n",
      "450_TRANSCRIPT.csv\n",
      "453_TRANSCRIPT.csv\n",
      "440_TRANSCRIPT.csv\n",
      "432_TRANSCRIPT.csv\n",
      "304_TRANSCRIPT.csv\n",
      "478_TRANSCRIPT.csv\n",
      "364_TRANSCRIPT.csv\n",
      "319_TRANSCRIPT.csv\n",
      "362_TRANSCRIPT.csv\n",
      "347_TRANSCRIPT.csv\n",
      "305_TRANSCRIPT.csv\n",
      "437_TRANSCRIPT.csv\n",
      "301_TRANSCRIPT.csv\n",
      "358_TRANSCRIPT.csv\n",
      "307_TRANSCRIPT.csv\n",
      "348_TRANSCRIPT.csv\n",
      "309_TRANSCRIPT.csv\n",
      "418_TRANSCRIPT.csv\n",
      "416_TRANSCRIPT.csv\n",
      "File was created\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/nevronas/IG-GPUshare/Amit/transcripts/\"\n",
    "transcripts_to_dataframe(data_path) \n",
    "all_participants = pd.read_csv(data_path + 'all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>personId</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>hi i'm ellie thanks for coming in today i was ...</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>472</td>\n",
       "      <td>okay</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>472</td>\n",
       "      <td>so how are you doing today</td>\n",
       "      <td>i'm well thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>472</td>\n",
       "      <td>that's good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>472</td>\n",
       "      <td>where are you from originally</td>\n",
       "      <td>southern california</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  personId                                           question  \\\n",
       "0           0       472  hi i'm ellie thanks for coming in today i was ...   \n",
       "1           1       472                                               okay   \n",
       "2           2       472                         so how are you doing today   \n",
       "3           3       472                                        that's good   \n",
       "4           4       472                      where are you from originally   \n",
       "\n",
       "                 answer  \n",
       "0                  sure  \n",
       "1                   NaN  \n",
       "2    i'm well thank you  \n",
       "3                   NaN  \n",
       "4   southern california  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_participants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants.columns =  ['index','personId', 'question', 'answer']\n",
    "all_participants = all_participants.astype({\"index\": int, \"personId\": float, \"question\": str, \"answer\": str })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):    \n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    else:\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    \n",
    "    text = re.sub(r\"\\<\", \" \", text)\n",
    "    text = re.sub(r\"\\>\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nevronas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nevronas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>personId</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>hi i'm ellie thanks for coming in today i was ...</td>\n",
       "      <td>[sure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>472.0</td>\n",
       "      <td>okay</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>472.0</td>\n",
       "      <td>so how are you doing today</td>\n",
       "      <td>[i, am, well, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>472.0</td>\n",
       "      <td>that's good</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>472.0</td>\n",
       "      <td>where are you from originally</td>\n",
       "      <td>[southern, california]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  personId                                           question  \\\n",
       "0      0     472.0  hi i'm ellie thanks for coming in today i was ...   \n",
       "1      1     472.0                                               okay   \n",
       "2      2     472.0                         so how are you doing today   \n",
       "3      3     472.0                                        that's good   \n",
       "4      4     472.0                      where are you from originally   \n",
       "\n",
       "                   answer  \n",
       "0                  [sure]  \n",
       "1                      []  \n",
       "2    [i, am, well, thank]  \n",
       "3                      []  \n",
       "4  [southern, california]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_participants_mix = all_participants.copy()\n",
    "all_participants_mix['answer'] = all_participants_mix.apply(lambda row: text_to_wordlist(row.answer).split(), axis=1)\n",
    "all_participants_mix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_mix_stopwords = all_participants.copy()\n",
    "all_participants_mix_stopwords['answer'] = all_participants_mix_stopwords.apply(lambda row: text_to_wordlist(row.answer, remove_stopwords=False).split(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7373\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in all_participants_mix['answer'].tolist()]\n",
    "words = set(itertools.chain(*words))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stop = [w for w in all_participants_mix_stopwords['answer'].tolist()]\n",
    "words_stop = set(itertools.chain(*words_stop))\n",
    "vocab_size_stop = len(words_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>personId</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>hi i'm ellie thanks for coming in today i was ...</td>\n",
       "      <td>[sure]</td>\n",
       "      <td>[97]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>472.0</td>\n",
       "      <td>okay</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>472.0</td>\n",
       "      <td>so how are you doing today</td>\n",
       "      <td>[i, am, well, thank]</td>\n",
       "      <td>[3, 6, 15, 173]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>472.0</td>\n",
       "      <td>that's good</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>472.0</td>\n",
       "      <td>where are you from originally</td>\n",
       "      <td>[southern, california]</td>\n",
       "      <td>[1074, 233]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  personId                                           question  \\\n",
       "0      0     472.0  hi i'm ellie thanks for coming in today i was ...   \n",
       "1      1     472.0                                               okay   \n",
       "2      2     472.0                         so how are you doing today   \n",
       "3      3     472.0                                        that's good   \n",
       "4      4     472.0                      where are you from originally   \n",
       "\n",
       "                   answer         t_answer  \n",
       "0                  [sure]             [97]  \n",
       "1                      []               []  \n",
       "2    [i, am, well, thank]  [3, 6, 15, 173]  \n",
       "3                      []               []  \n",
       "4  [southern, california]      [1074, 233]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(all_participants_mix['answer'])\n",
    "tokenizer.fit_on_sequences(all_participants_mix['answer'])\n",
    "\n",
    "all_participants_mix['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix['answer'])\n",
    "all_participants_mix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>personId</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>hi i'm ellie thanks for coming in today i was ...</td>\n",
       "      <td>[sure]</td>\n",
       "      <td>[163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>472.0</td>\n",
       "      <td>okay</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>472.0</td>\n",
       "      <td>so how are you doing today</td>\n",
       "      <td>[i, am, well, thank, you]</td>\n",
       "      <td>[1, 18, 41, 254, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>472.0</td>\n",
       "      <td>that's good</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>472.0</td>\n",
       "      <td>where are you from originally</td>\n",
       "      <td>[southern, california]</td>\n",
       "      <td>[1167, 319]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  personId                                           question  \\\n",
       "0      0     472.0  hi i'm ellie thanks for coming in today i was ...   \n",
       "1      1     472.0                                               okay   \n",
       "2      2     472.0                         so how are you doing today   \n",
       "3      3     472.0                                        that's good   \n",
       "4      4     472.0                      where are you from originally   \n",
       "\n",
       "                      answer              t_answer  \n",
       "0                     [sure]                 [163]  \n",
       "1                         []                    []  \n",
       "2  [i, am, well, thank, you]  [1, 18, 41, 254, 11]  \n",
       "3                         []                    []  \n",
       "4     [southern, california]           [1167, 319]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "tokenizer = Tokenizer(num_words=vocab_size_stop)\n",
    "tokenizer.fit_on_texts(all_participants_mix_stopwords['answer'])\n",
    "tokenizer.fit_on_sequences(all_participants_mix_stopwords['answer'])\n",
    "\n",
    "all_participants_mix_stopwords['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix_stopwords['answer'])\n",
    "all_participants_mix_stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_index = tokenizer.word_index\n",
    "word_size = len(word_index)\n",
    "print(word_index[\"sad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personId\n",
      "300.0    [good, atlanta, georgia, um, my, parent, are, ...\n",
      "301.0    [thank, you, mmm, k, i, am, doing, good, thank...\n",
      "302.0    [i, am, fine, how, about, yourself, i, am, fro...\n",
      "303.0    [okay, how, bout, yourself, here, in, californ...\n",
      "304.0    [i, am, doing, good, um, from, los, angeles, c...\n",
      "305.0    [i, am, doing, alright, uh, originally, i, am,...\n",
      "306.0    [fine, uh, colorado, mhm, uh, career, career, ...\n",
      "307.0    [laughter, um, moscow, um, my, family, moved, ...\n",
      "308.0    [los, angeles, california, yes, um, the, south...\n",
      "309.0    [laughter, laughter, yeah, laughter, laughter,...\n",
      "310.0    [yes, it, okay, laughter, fine, laughter, i, l...\n",
      "311.0    [yes, okay, uh, when, would, i, move, to, l, a...\n",
      "312.0    [yes, fine, how, about, you, here, yes, the, w...\n",
      "313.0    [sure, uh, okay, i, guess, not, bad, not, good...\n",
      "314.0    [yes, quite, well, feel, good, los, angeles, c...\n",
      "315.0    [alright, yes, okay, and, you, inglewood, cali...\n",
      "316.0    [yes, i, am, fine, new, york, uh, for, busines...\n",
      "317.0    [yes, uh, i, am, okay, a, little, sleepy, righ...\n",
      "318.0    [yes, i, am, alright, los, angeles, california...\n",
      "319.0    [sure, mm, okay, los, angeles, um, well, laugh...\n",
      "320.0    [yes, i, am, a, little, nervous, mm, this, is,...\n",
      "321.0    [yeah, i, am, okay, um, chicago, illinois, mhm...\n",
      "322.0    [yes, fine, thank, you, i, am, originally, fro...\n",
      "323.0    [sure, good, chicago, it, cold, in, chicago, h...\n",
      "324.0    [yeah, yes, fine, how, are, you, uh, born, and...\n",
      "325.0    [yes, um, i, am, fine, this, is, a, little, bi...\n",
      "326.0    [yes, i, am, fine, um, ohio, mmm, laughter, bi...\n",
      "327.0    [yes, fine, thank, you, montreal, canada, i, m...\n",
      "328.0    [yes, laughter, fine, how, are, you, uh, watt,...\n",
      "329.0    [yes, pretty, well, from, new, york, uh, to, g...\n",
      "                               ...                        \n",
      "462.0    [yeah, i, am, okay, uh, i, am, from, los, ange...\n",
      "463.0    [yes, i, am, i, am, pretty, good, uh, i, wa, b...\n",
      "464.0    [mhm, um, i, am, doing, good, what, about, you...\n",
      "465.0    [yeah, cool, i, am, hanging, out, you, know, b...\n",
      "466.0    [yes, uh, okay, laughter, los, angeles, the, w...\n",
      "467.0    [yeah, i, am, okay, with, it, uh, i, feel, i, ...\n",
      "468.0    [yes, i, am, okay, with, this, i, am, pretty, ...\n",
      "469.0    [yes, i, am, okay, l, a, mm, uh, there, a, lot...\n",
      "470.0    [yes, i, am, doing, okay, i, am, from, los, an...\n",
      "471.0    [yes, i, am, doing, well, thank, you, from, lo...\n",
      "472.0    [sure, i, am, well, thank, you, southern, cali...\n",
      "473.0    [yes, good, new, jersey, five, year, ago, abou...\n",
      "474.0    [yes, i, am, good, how, are, you, uh, i, wa, b...\n",
      "475.0    [yes, good, thank, you, el, segundo, right, do...\n",
      "476.0    [uh, yeah, sure, laughter, i, am, alright, uh,...\n",
      "477.0    [yes, thanks, i, am, well, thank, you, how, ar...\n",
      "478.0    [yes, uh, good, thanks, the, east, coast, for,...\n",
      "479.0    [yes, okay, l, a, uh, the, smog, traffic, tick...\n",
      "481.0    [yeah, i, am, um, good, to, moderate, los, ang...\n",
      "482.0    [yes, i, am, doing, fine, i, wa, born, in, tex...\n",
      "483.0    [sure, laughter, mm, good, i, guess, new, orle...\n",
      "484.0    [yes, i, am, doing, well, i, wa, born, in, oak...\n",
      "485.0    [yes, i, am, not, bad, i, am, a, little, tired...\n",
      "486.0    [yes, i, am, feel, great, i, am, from, saint, ...\n",
      "487.0    [yes, i, am, fine, thank, you, detroit, michig...\n",
      "488.0    [yes, fine, oh, san, fernando, valley, uh, wel...\n",
      "489.0    [yes, i, am, doing, well, thank, you, san, lui...\n",
      "490.0    [yeah, i, am, doing, already, how, are, you, d...\n",
      "491.0    [yes, huh, overwhelmed, i, have, a, funeral, t...\n",
      "492.0    [yes, doing, pretty, good, thank, you, marylan...\n",
      "Name: answer, Length: 186, dtype: object\n"
     ]
    }
   ],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "cont = 0\n",
    "word_index = tokenizer\n",
    "phrases_lp_stop = pd.DataFrame(columns=['personId','answer', 't_answer'])\n",
    "answers = all_participants_mix_stopwords.groupby('personId').agg('sum', axis=1)\n",
    "\n",
    "print(answers[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "366\n",
      "1910\n",
      "2537\n",
      "4564\n",
      "5586\n",
      "9031\n",
      "10645\n",
      "13317\n",
      "14253\n",
      "14978\n",
      "16167\n",
      "16840\n",
      "18094\n",
      "18864\n",
      "22270\n",
      "23681\n",
      "24333\n",
      "25004\n",
      "25734\n",
      "26438\n",
      "27229\n",
      "28181\n",
      "29783\n",
      "31179\n",
      "32089\n",
      "33961\n",
      "34490\n",
      "35308\n",
      "37387\n",
      "38393\n",
      "38893\n",
      "40421\n",
      "41348\n",
      "42739\n",
      "44279\n",
      "45715\n",
      "46737\n",
      "51500\n",
      "52373\n",
      "53416\n",
      "54025\n",
      "55336\n",
      "56086\n",
      "58111\n",
      "59615\n",
      "61954\n",
      "62412\n",
      "63425\n",
      "64967\n",
      "66450\n",
      "67651\n",
      "69384\n",
      "70677\n",
      "71189\n",
      "72243\n",
      "73920\n",
      "74343\n",
      "75023\n",
      "77376\n",
      "78019\n",
      "79352\n",
      "80184\n",
      "82410\n",
      "86641\n",
      "89041\n",
      "91691\n",
      "94841\n",
      "98498\n",
      "100818\n",
      "104009\n",
      "104853\n",
      "107126\n",
      "110843\n",
      "113307\n",
      "113865\n",
      "115659\n",
      "118143\n",
      "119495\n",
      "122094\n",
      "126155\n",
      "127879\n",
      "129032\n",
      "131212\n",
      "132345\n",
      "132527\n",
      "134731\n",
      "135297\n",
      "135775\n",
      "136363\n",
      "137814\n",
      "138437\n",
      "139156\n",
      "139911\n",
      "141173\n",
      "142312\n",
      "144098\n",
      "145299\n",
      "146517\n",
      "147752\n",
      "148927\n",
      "150409\n",
      "151498\n",
      "154051\n",
      "155105\n",
      "158365\n",
      "159245\n",
      "161725\n",
      "163222\n",
      "166294\n",
      "167179\n",
      "168929\n",
      "170431\n",
      "171685\n",
      "173201\n",
      "174465\n",
      "176141\n",
      "177836\n",
      "178954\n",
      "180049\n",
      "182566\n",
      "183609\n",
      "186170\n",
      "187978\n",
      "189206\n",
      "190309\n",
      "191330\n",
      "192564\n",
      "194061\n",
      "194949\n",
      "196252\n",
      "197200\n",
      "199869\n",
      "200911\n",
      "202344\n",
      "203590\n",
      "206969\n",
      "210093\n",
      "211990\n",
      "212964\n",
      "214603\n",
      "215140\n",
      "218037\n",
      "219080\n",
      "220873\n",
      "222305\n",
      "224942\n",
      "228109\n",
      "230186\n",
      "230789\n",
      "232313\n",
      "233009\n",
      "234107\n",
      "235798\n",
      "237199\n",
      "238468\n",
      "239491\n",
      "241575\n",
      "242517\n",
      "244582\n",
      "247365\n",
      "249856\n",
      "251134\n",
      "253077\n",
      "254804\n",
      "256281\n",
      "257797\n",
      "258720\n",
      "259065\n",
      "260863\n",
      "261605\n",
      "262265\n",
      "265148\n",
      "266503\n",
      "267189\n",
      "269271\n",
      "271219\n",
      "273588\n",
      "275009\n",
      "275735\n",
      "276521\n",
      "278376\n",
      "279838\n",
      "280361\n",
      "280972\n",
      "282241\n"
     ]
    }
   ],
   "source": [
    "cont = 0\n",
    "for p in answers.iterrows():      \n",
    "    words = p[1][\"answer\"]\n",
    "    size = len(words)\n",
    "    word_tokens = p[1][\"t_answer\"]\n",
    "    print(cont)\n",
    "    for i in range(size):\n",
    "        sentence = words[i:min(i+windows_size,size)]  \n",
    "        tokens = word_tokens[i:min(i+windows_size,size)]  \n",
    "        phrases_lp_stop.loc[cont] = [p[0], sentence, tokens]\n",
    "        cont = cont + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[good, atlanta, georgia, um, my, parent, are, ...</td>\n",
       "      <td>[42, 1732, 2098, 5, 12, 205, 37, 69, 113, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[atlanta, georgia, um, my, parent, are, from, ...</td>\n",
       "      <td>[1732, 2098, 5, 12, 205, 37, 69, 113, 5, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[georgia, um, my, parent, are, from, here, um,...</td>\n",
       "      <td>[2098, 5, 12, 205, 37, 69, 113, 5, 1, 119]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[um, my, parent, are, from, here, um, i, love,...</td>\n",
       "      <td>[5, 12, 205, 37, 69, 113, 5, 1, 119, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[my, parent, are, from, here, um, i, love, it, i]</td>\n",
       "      <td>[12, 205, 37, 69, 113, 5, 1, 119, 6, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                             answer  \\\n",
       "0     300.0  [good, atlanta, georgia, um, my, parent, are, ...   \n",
       "1     300.0  [atlanta, georgia, um, my, parent, are, from, ...   \n",
       "2     300.0  [georgia, um, my, parent, are, from, here, um,...   \n",
       "3     300.0  [um, my, parent, are, from, here, um, i, love,...   \n",
       "4     300.0  [my, parent, are, from, here, um, i, love, it, i]   \n",
       "\n",
       "                                       t_answer  \n",
       "0  [42, 1732, 2098, 5, 12, 205, 37, 69, 113, 5]  \n",
       "1   [1732, 2098, 5, 12, 205, 37, 69, 113, 5, 1]  \n",
       "2    [2098, 5, 12, 205, 37, 69, 113, 5, 1, 119]  \n",
       "3       [5, 12, 205, 37, 69, 113, 5, 1, 119, 6]  \n",
       "4       [12, 205, 37, 69, 113, 5, 1, 119, 6, 1]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_lp_stop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "216\n",
      "948\n",
      "1289\n",
      "2306\n",
      "2820\n",
      "4467\n",
      "5272\n",
      "6634\n",
      "7128\n",
      "7522\n",
      "8134\n",
      "8492\n",
      "9114\n",
      "9541\n",
      "11143\n",
      "11859\n",
      "12199\n",
      "12612\n",
      "12990\n",
      "13364\n",
      "13740\n",
      "14193\n",
      "15043\n",
      "15774\n",
      "16260\n",
      "17226\n",
      "17541\n",
      "17988\n",
      "18998\n",
      "19510\n",
      "19807\n",
      "20537\n",
      "21021\n",
      "21756\n",
      "22506\n",
      "23241\n",
      "23777\n",
      "25875\n",
      "26337\n",
      "26890\n",
      "27214\n",
      "27861\n",
      "28208\n",
      "29187\n",
      "30033\n",
      "31162\n",
      "31393\n",
      "31906\n",
      "32726\n",
      "33442\n",
      "34097\n",
      "34961\n",
      "35630\n",
      "35916\n",
      "36462\n",
      "37272\n",
      "37520\n",
      "37921\n",
      "39117\n",
      "39453\n",
      "40176\n",
      "40612\n",
      "41633\n",
      "43633\n",
      "44724\n",
      "46096\n",
      "47446\n",
      "49283\n",
      "50470\n",
      "52061\n",
      "52512\n",
      "53621\n",
      "55410\n",
      "56647\n",
      "56942\n",
      "57855\n",
      "59048\n",
      "59743\n",
      "61122\n",
      "63171\n",
      "63974\n",
      "64588\n",
      "65735\n",
      "66337\n",
      "66430\n",
      "67611\n",
      "67925\n",
      "68167\n",
      "68473\n",
      "69276\n",
      "69612\n",
      "69995\n",
      "70382\n",
      "71056\n",
      "71623\n",
      "72482\n",
      "73094\n",
      "73766\n",
      "74375\n",
      "74966\n",
      "75725\n",
      "76316\n",
      "77554\n",
      "78141\n",
      "79799\n",
      "80270\n",
      "81574\n",
      "82292\n",
      "83751\n",
      "84254\n",
      "85067\n",
      "85771\n",
      "86414\n",
      "87162\n",
      "87834\n",
      "88666\n",
      "89515\n",
      "90086\n",
      "90618\n",
      "91842\n",
      "92382\n",
      "93546\n",
      "94464\n",
      "95095\n",
      "95675\n",
      "96222\n",
      "96897\n",
      "97670\n",
      "98152\n",
      "98855\n",
      "99358\n",
      "100583\n",
      "101171\n",
      "101863\n",
      "102549\n",
      "104204\n",
      "105711\n",
      "106599\n",
      "107096\n",
      "107951\n",
      "108286\n",
      "109691\n",
      "110175\n",
      "111096\n",
      "111794\n",
      "113099\n",
      "114452\n",
      "115480\n",
      "115793\n",
      "116535\n",
      "116938\n",
      "117489\n",
      "118336\n",
      "119083\n",
      "119717\n",
      "120237\n",
      "121265\n",
      "121723\n",
      "122828\n",
      "124247\n",
      "125620\n",
      "126288\n",
      "127310\n",
      "128287\n",
      "128992\n",
      "129770\n",
      "130250\n",
      "130436\n",
      "131292\n",
      "131710\n",
      "132086\n",
      "133450\n",
      "134128\n",
      "134516\n",
      "135551\n",
      "136473\n",
      "137754\n",
      "138472\n",
      "138900\n",
      "139305\n",
      "140215\n",
      "140930\n",
      "141222\n",
      "141533\n",
      "142171\n"
     ]
    }
   ],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "cont = 0\n",
    "word_index = tokenizer\n",
    "phrases_lp = pd.DataFrame(columns=['personId','answer', 't_answer'])\n",
    "answers = all_participants_mix.groupby('personId').agg('sum', axis=1)\n",
    "\n",
    "for p in answers.iterrows():      \n",
    "    \n",
    "    words = p[1][\"answer\"]\n",
    "    size = len(words)\n",
    "    word_tokens = p[1][\"t_answer\"]\n",
    "    print(cont)\n",
    "    for i in range(size):\n",
    "        sentence = words[i:min(i+windows_size,size)]  \n",
    "        tokens = word_tokens[i:min(i+windows_size,size)]  \n",
    "        phrases_lp.loc[cont] = [p[0], sentence, tokens]\n",
    "        cont = cont + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "phrases_lp.to_csv(data_path + 'phrases_lp.csv', sep='\\t')\n",
    "print(\"File was created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[good, atlanta, georgia, um, parent, um, love,...</td>\n",
       "      <td>[16, 1639, 2007, 1, 131, 1, 63, 5, 142, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[atlanta, georgia, um, parent, um, love, like,...</td>\n",
       "      <td>[1639, 2007, 1, 131, 1, 63, 5, 142, 5, 336]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[georgia, um, parent, um, love, like, weather,...</td>\n",
       "      <td>[2007, 1, 131, 1, 63, 5, 142, 5, 336, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[um, parent, um, love, like, weather, like, op...</td>\n",
       "      <td>[1, 131, 1, 63, 5, 142, 5, 336, 1, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[parent, um, love, like, weather, like, opport...</td>\n",
       "      <td>[131, 1, 63, 5, 142, 5, 336, 1, 39, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                             answer  \\\n",
       "0     300.0  [good, atlanta, georgia, um, parent, um, love,...   \n",
       "1     300.0  [atlanta, georgia, um, parent, um, love, like,...   \n",
       "2     300.0  [georgia, um, parent, um, love, like, weather,...   \n",
       "3     300.0  [um, parent, um, love, like, weather, like, op...   \n",
       "4     300.0  [parent, um, love, like, weather, like, opport...   \n",
       "\n",
       "                                      t_answer  \n",
       "0   [16, 1639, 2007, 1, 131, 1, 63, 5, 142, 5]  \n",
       "1  [1639, 2007, 1, 131, 1, 63, 5, 142, 5, 336]  \n",
       "2     [2007, 1, 131, 1, 63, 5, 142, 5, 336, 1]  \n",
       "3       [1, 131, 1, 63, 5, 142, 5, 336, 1, 39]  \n",
       "4       [131, 1, 63, 5, 142, 5, 336, 1, 39, 1]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[good, atlanta, georgia, um, parent, um, love,...</td>\n",
       "      <td>[16, 1639, 2007, 1, 131, 1, 63, 5, 142, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[atlanta, georgia, um, parent, um, love, like,...</td>\n",
       "      <td>[1639, 2007, 1, 131, 1, 63, 5, 142, 5, 336]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[georgia, um, parent, um, love, like, weather,...</td>\n",
       "      <td>[2007, 1, 131, 1, 63, 5, 142, 5, 336, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[um, parent, um, love, like, weather, like, op...</td>\n",
       "      <td>[1, 131, 1, 63, 5, 142, 5, 336, 1, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300.0</td>\n",
       "      <td>[parent, um, love, like, weather, like, opport...</td>\n",
       "      <td>[131, 1, 63, 5, 142, 5, 336, 1, 39, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                             answer  \\\n",
       "0     300.0  [good, atlanta, georgia, um, parent, um, love,...   \n",
       "1     300.0  [atlanta, georgia, um, parent, um, love, like,...   \n",
       "2     300.0  [georgia, um, parent, um, love, like, weather,...   \n",
       "3     300.0  [um, parent, um, love, like, weather, like, op...   \n",
       "4     300.0  [parent, um, love, like, weather, like, opport...   \n",
       "\n",
       "                                      t_answer  \n",
       "0   [16, 1639, 2007, 1, 131, 1, 63, 5, 142, 5]  \n",
       "1  [1639, 2007, 1, 131, 1, 63, 5, 142, 5, 336]  \n",
       "2     [2007, 1, 131, 1, 63, 5, 142, 5, 336, 1]  \n",
       "3       [1, 131, 1, 63, 5, 142, 5, 336, 1, 39]  \n",
       "4       [131, 1, 63, 5, 142, 5, 336, 1, 39, 1]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_lp[\"t_answer\"] = pad_sequences(phrases_lp[\"t_answer\"], value=0, padding=\"post\", maxlen=windows_size).tolist()\n",
    "phrases_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was created\n"
     ]
    }
   ],
   "source": [
    "phrases_lp_stop.to_csv(data_path + 'phrases_lp_stop.csv', sep='\\t')\n",
    "print(\"File was created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>['good', 'atlanta', 'georgia', 'um', 'parent',...</td>\n",
       "      <td>[16, 1639, 2007, 1, 131, 1, 63, 5, 142, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>['atlanta', 'georgia', 'um', 'parent', 'um', '...</td>\n",
       "      <td>[1639, 2007, 1, 131, 1, 63, 5, 142, 5, 336]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>['georgia', 'um', 'parent', 'um', 'love', 'lik...</td>\n",
       "      <td>[2007, 1, 131, 1, 63, 5, 142, 5, 336, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>['um', 'parent', 'um', 'love', 'like', 'weathe...</td>\n",
       "      <td>[1, 131, 1, 63, 5, 142, 5, 336, 1, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>300.0</td>\n",
       "      <td>['parent', 'um', 'love', 'like', 'weather', 'l...</td>\n",
       "      <td>[131, 1, 63, 5, 142, 5, 336, 1, 39, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  personId                                             answer  \\\n",
       "0           0     300.0  ['good', 'atlanta', 'georgia', 'um', 'parent',...   \n",
       "1           1     300.0  ['atlanta', 'georgia', 'um', 'parent', 'um', '...   \n",
       "2           2     300.0  ['georgia', 'um', 'parent', 'um', 'love', 'lik...   \n",
       "3           3     300.0  ['um', 'parent', 'um', 'love', 'like', 'weathe...   \n",
       "4           4     300.0  ['parent', 'um', 'love', 'like', 'weather', 'l...   \n",
       "\n",
       "                                      t_answer  \n",
       "0   [16, 1639, 2007, 1, 131, 1, 63, 5, 142, 5]  \n",
       "1  [1639, 2007, 1, 131, 1, 63, 5, 142, 5, 336]  \n",
       "2     [2007, 1, 131, 1, 63, 5, 142, 5, 336, 1]  \n",
       "3       [1, 131, 1, 63, 5, 142, 5, 336, 1, 39]  \n",
       "4       [131, 1, 63, 5, 142, 5, 336, 1, 39, 1]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_lp = pd.read_csv(data_path + 'phrases_lp.csv', sep='\\t', converters={\"t_answer\": literal_eval}) \n",
    "phrases_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_avec_dataset_file(path, score_column):\n",
    "    ds = pd.read_csv(path, sep=',')\n",
    "    ds['level'] = pd.cut(ds[score_column], bins=[-1,0,5,10,15,25], labels=[0,1,2,3,4])\n",
    "    ds['PHQ8_Score'] = ds[score_column]\n",
    "    ds['cat_level'] = keras.utils.to_categorical(ds['level'], num_classes).tolist()\n",
    "    ds = ds[['Participant_ID', 'level', 'cat_level', 'PHQ8_Score']]\n",
    "    ds = ds.astype({\"Participant_ID\": float, \"level\": int, 'PHQ8_Score': int})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: train= 107, dev= 35, test= 47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>level</th>\n",
       "      <th>cat_level</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID  level                  cat_level  PHQ8_Score\n",
       "0           303.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0\n",
       "1           304.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           6\n",
       "2           305.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           7\n",
       "3           310.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           4\n",
       "4           312.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = load_avec_dataset_file('/home/nevronas/IG-GPUshare/Amit/train_split_Depression_AVEC2017.csv', 'PHQ8_Score')\n",
    "dev = load_avec_dataset_file('/home/nevronas/IG-GPUshare/Amit/dev_split_Depression_AVEC2017.csv', 'PHQ8_Score')\n",
    "test = load_avec_dataset_file('/home/nevronas/IG-GPUshare/Amit/full_test_split.csv', 'PHQ_Score')\n",
    "print(\"Size: train= {}, dev= {}, test= {}\".format(len(train), len(dev), len(test)))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size = 189\n"
     ]
    }
   ],
   "source": [
    "ds_total = pd.concat([train,dev,test])\n",
    "total_phq8 = len(ds_total)\n",
    "print(\"Total size = {}\".format(total_phq8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "ds_total.to_csv(data_path + 'ds_total.csv', sep = '\\t')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_phq_level(ds):\n",
    "    none_ds = ds[ds['level']==0]\n",
    "    mild_ds = ds[ds['level']==1]\n",
    "    moderate_ds = ds[ds['level']==2]\n",
    "    moderate_severe_ds = ds[ds['level']==3]\n",
    "    severe_ds = ds[ds['level']==4]\n",
    "    return (none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity per none_ds: 26, mild_ds: 70, moderate_ds 47, moderate_severe_ds: 24, severe_ds 22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_total)\n",
    "print(\"Quantity per none_ds: {}, mild_ds: {}, moderate_ds {}, moderate_severe_ds: {}, severe_ds {}\".format(len(none_ds), len(mild_ds), len(moderate_ds), len(moderate_severe_ds), len(severe_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity per none_ds: 26, mild_ds: 26, moderate_ds 26, moderate_severe_ds: 24, severe_ds 22\n"
     ]
    }
   ],
   "source": [
    "b_none_ds = ds_total[ds_total['level']==0]\n",
    "b_mild_ds = ds_total[ds_total['level']==1].sample(26)\n",
    "b_moderate_ds = ds_total[ds_total['level']==2].sample(26)\n",
    "b_moderate_severe_ds = ds_total[ds_total['level']==3]\n",
    "b_severe_ds = ds_total[ds_total['level']==4]\n",
    "\n",
    "ds_total_b = pd.concat([b_none_ds, b_mild_ds, b_moderate_ds, b_moderate_severe_ds, b_severe_ds])\n",
    "print(\"Quantity per none_ds: {}, mild_ds: {}, moderate_ds {}, moderate_severe_ds: {}, severe_ds {}\".format(len(b_none_ds), len(b_mild_ds), len(b_moderate_ds), len(b_moderate_severe_ds), len(b_severe_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lp = pd.merge(ds_total, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
    "ds_lp_b = pd.merge(ds_total_b, phrases_lp,left_on='Participant_ID', right_on='personId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_instances(ds, split_in = [70,14,16]):\n",
    "    ds_shuffled = ds.sample(frac=1)\n",
    "    none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_shuffled)\n",
    "    eq_ds = dict()\n",
    "    prev_none = prev_mild = prev_moderate = prev_moderate_severe = prev_severe = 0\n",
    "    split = split_in\n",
    "    for p in split:\n",
    "        last_none = min(len(none_ds), prev_none + round(len(none_ds) * p/100))\n",
    "        last_mild = min(len(mild_ds), prev_mild + round(len(mild_ds) * p/100))\n",
    "        last_moderate = min(len(moderate_ds), prev_moderate + round(len(moderate_ds) * p/100))\n",
    "        last_moderate_severe = min(len(moderate_severe_ds), prev_moderate_severe + round(len(moderate_severe_ds) * p/100))\n",
    "        last_severe = min(len(severe_ds), prev_severe + round(len(severe_ds) * p/100))  \n",
    "        eq_ds['d'+str(p)] = pd.concat([none_ds[prev_none: last_none], mild_ds[prev_mild: last_mild], moderate_ds[prev_moderate: last_moderate], moderate_severe_ds[prev_moderate_severe: last_moderate_severe], severe_ds[prev_severe: last_severe]])\n",
    "        prev_none = last_none\n",
    "        prev_mild = last_mild\n",
    "        prev_moderate = last_moderate\n",
    "        prev_moderate_severe = last_moderate_severe\n",
    "        prev_severe = last_severe  \n",
    "    return (eq_ds['d70'], eq_ds['d14'], eq_ds['d16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp, dev_lp, test_lp = distribute_instances(ds_lp)\n",
    "train_lp_b, dev_lp_b, test_lp_b = distribute_instances(ds_lp_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>level</th>\n",
       "      <th>cat_level</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48688</th>\n",
       "      <td>414.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>85135</td>\n",
       "      <td>414.0</td>\n",
       "      <td>['i', 'have', 'read', 'seen', 'picture', 'um',...</td>\n",
       "      <td>[3, 13, 283, 341, 838, 1, 338, 225, 239, 830]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49179</th>\n",
       "      <td>414.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>85626</td>\n",
       "      <td>414.0</td>\n",
       "      <td>['laughter', 'um', 'probably', 'super', 'outgo...</td>\n",
       "      <td>[8, 1, 27, 673, 155, 5, 1156, 639, 328, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135431</th>\n",
       "      <td>461.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>120061</td>\n",
       "      <td>461.0</td>\n",
       "      <td>['well', 'know', 'diagnosis', 'xxx', 'thirty',...</td>\n",
       "      <td>[15, 4, 1574, 152, 477, 22, 58, 152, 477, 213]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84532</th>\n",
       "      <td>377.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>58933</td>\n",
       "      <td>377.0</td>\n",
       "      <td>['supportive', 'loving', 'nurturing', 'um', 'i...</td>\n",
       "      <td>[903, 700, 1956, 1, 3, 6, 100, 91, 3, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53365</th>\n",
       "      <td>426.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>94807</td>\n",
       "      <td>426.0</td>\n",
       "      <td>['age', 'finishing', 'school', 'getting', 'mas...</td>\n",
       "      <td>[467, 1343, 59, 81, 1067, 530, 16, 71, 107, 6213]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_ID  level                  cat_level  PHQ8_Score  \\\n",
       "48688            414.0      4  [0.0, 0.0, 0.0, 0.0, 1.0]          16   \n",
       "49179            414.0      4  [0.0, 0.0, 0.0, 0.0, 1.0]          16   \n",
       "135431           461.0      4  [0.0, 0.0, 0.0, 0.0, 1.0]          17   \n",
       "84532            377.0      4  [0.0, 0.0, 0.0, 0.0, 1.0]          16   \n",
       "53365            426.0      4  [0.0, 0.0, 0.0, 0.0, 1.0]          20   \n",
       "\n",
       "        Unnamed: 0  personId  \\\n",
       "48688        85135     414.0   \n",
       "49179        85626     414.0   \n",
       "135431      120061     461.0   \n",
       "84532        58933     377.0   \n",
       "53365        94807     426.0   \n",
       "\n",
       "                                                   answer  \\\n",
       "48688   ['i', 'have', 'read', 'seen', 'picture', 'um',...   \n",
       "49179   ['laughter', 'um', 'probably', 'super', 'outgo...   \n",
       "135431  ['well', 'know', 'diagnosis', 'xxx', 'thirty',...   \n",
       "84532   ['supportive', 'loving', 'nurturing', 'um', 'i...   \n",
       "53365   ['age', 'finishing', 'school', 'getting', 'mas...   \n",
       "\n",
       "                                                 t_answer  \n",
       "48688       [3, 13, 283, 341, 838, 1, 338, 225, 239, 830]  \n",
       "49179         [8, 1, 27, 673, 155, 5, 1156, 639, 328, 30]  \n",
       "135431     [15, 4, 1574, 152, 477, 22, 58, 152, 477, 213]  \n",
       "84532            [903, 700, 1956, 1, 3, 6, 100, 91, 3, 6]  \n",
       "53365   [467, 1343, 59, 81, 1067, 530, 16, 71, 107, 6213]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(text, model):\n",
    "    print(text)\n",
    "    word_list = text_to_wordlist(text)\n",
    "    sequences = tokenizer.texts_to_sequences([word_list])\n",
    "    sequences_input = list(itertools.chain(*sequences))\n",
    "    sequences_input =  pad_sequences([sequences_input], value=0, padding=\"post\", maxlen=windows_size).tolist()\n",
    "    input_a = np.asarray(sequences_input)\n",
    "    pred = model.predict(input_a, batch_size=None, verbose=0, steps=None)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    print(labels[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model, x, y):\n",
    "    prediction = model.predict(x, batch_size=None, verbose=0, steps=None)\n",
    "    labels=['none','mild','moderate','moderately severe', 'severe']\n",
    "\n",
    "    max_prediction = np.argmax(prediction, axis=1)\n",
    "    max_actual = np.argmax(y, axis=1)\n",
    "\n",
    "    y_pred = pd.Categorical.from_codes(max_prediction, labels)\n",
    "    y_actu = pd.Categorical.from_codes(max_actual, labels)\n",
    "\n",
    "    return pd.crosstab(y_actu, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/home/nevronas/IG-GPUshare/Amit/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_embedding_matrix(tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "    embedding_matrix = np.zeros((vocab_size+1, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:        \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_lp = fill_embedding_matrix(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>level</th>\n",
       "      <th>cat_level</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25947</th>\n",
       "      <td>364.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>42445</td>\n",
       "      <td>364.0</td>\n",
       "      <td>['am', 'pretty', 'good', 'think', 'i', 'am', '...</td>\n",
       "      <td>[6, 23, 16, 12, 3, 6, 19, 80, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>303.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>2029</td>\n",
       "      <td>303.0</td>\n",
       "      <td>['talking', 'wall', 'opposite', 'regretting', ...</td>\n",
       "      <td>[195, 1078, 1042, 2135, 4, 49, 5, 177, 201, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139588</th>\n",
       "      <td>467.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>125781</td>\n",
       "      <td>467.0</td>\n",
       "      <td>['um', 'wanted', 'try', 'something', 'differen...</td>\n",
       "      <td>[1, 125, 62, 29, 38, 29, 3, 10, 85, 110]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71682</th>\n",
       "      <td>471.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>129267</td>\n",
       "      <td>471.0</td>\n",
       "      <td>['diffuses', 'trouble', 'rather', 'prolongs', ...</td>\n",
       "      <td>[4802, 315, 436, 4803, 4804, 315, 2, 40, 43, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26908</th>\n",
       "      <td>364.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>43406</td>\n",
       "      <td>364.0</td>\n",
       "      <td>['gonna', 'nice', 'day', 'looked', 'forward', ...</td>\n",
       "      <td>[104, 129, 42, 800, 524, 96, 905, 2, 36, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_ID  level                  cat_level  PHQ8_Score  \\\n",
       "25947            364.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0   \n",
       "740              303.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0   \n",
       "139588           467.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0   \n",
       "71682            471.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0   \n",
       "26908            364.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0   \n",
       "\n",
       "        Unnamed: 0  personId  \\\n",
       "25947        42445     364.0   \n",
       "740           2029     303.0   \n",
       "139588      125781     467.0   \n",
       "71682       129267     471.0   \n",
       "26908        43406     364.0   \n",
       "\n",
       "                                                   answer  \\\n",
       "25947   ['am', 'pretty', 'good', 'think', 'i', 'am', '...   \n",
       "740     ['talking', 'wall', 'opposite', 'regretting', ...   \n",
       "139588  ['um', 'wanted', 'try', 'something', 'differen...   \n",
       "71682   ['diffuses', 'trouble', 'rather', 'prolongs', ...   \n",
       "26908   ['gonna', 'nice', 'day', 'looked', 'forward', ...   \n",
       "\n",
       "                                                t_answer  \n",
       "25947               [6, 23, 16, 12, 3, 6, 19, 80, 15, 3]  \n",
       "740       [195, 1078, 1042, 2135, 4, 49, 5, 177, 201, 4]  \n",
       "139588          [1, 125, 62, 29, 38, 29, 3, 10, 85, 110]  \n",
       "71682   [4802, 315, 436, 4803, 4804, 315, 2, 40, 43, 10]  \n",
       "26908        [104, 129, 42, 800, 524, 96, 905, 2, 36, 3]  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99969\n",
      "98781\n"
     ]
    }
   ],
   "source": [
    "train_lp_copy = train_lp\n",
    "print(len(train_lp_copy))\n",
    "for index,row in train_lp_copy.iterrows():\n",
    "    if len(row['t_answer'])<10:\n",
    "        train_lp_copy.drop(index,inplace=True)\n",
    "print(len(train_lp_copy))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp = train_lp_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19993\n",
      "19767\n"
     ]
    }
   ],
   "source": [
    "dev_lp_copy = dev_lp\n",
    "print(len(dev_lp_copy))\n",
    "for index,row in dev_lp_copy.iterrows():\n",
    "    if len(row['t_answer'])<10:\n",
    "        dev_lp_copy.drop(index,inplace=True)\n",
    "print(len(dev_lp_copy))  \n",
    "dev_lp = dev_lp_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22849\n",
      "22588\n"
     ]
    }
   ],
   "source": [
    "test_lp_copy = test_lp\n",
    "print(len(test_lp_copy))\n",
    "for index,row in test_lp_copy.iterrows():\n",
    "    if len(row['t_answer'])<10:\n",
    "        test_lp_copy.drop(index,inplace=True)\n",
    "print(len(test_lp_copy))  \n",
    "test_lp = test_lp_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = np.stack(train_lp['t_answer'], axis=0)\n",
    "dev_a = np.stack(dev_lp['t_answer'], axis=0)\n",
    "train_y = np.stack(train_lp['cat_level'], axis=0)\n",
    "dev_y = np.stack(dev_lp['cat_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65594\n",
      "64835\n"
     ]
    }
   ],
   "source": [
    "train_lp_b_copy = train_lp_b\n",
    "print(len(train_lp_b_copy))\n",
    "for index,row in train_lp_b_copy.iterrows():\n",
    "    if len(row['t_answer'])<10:\n",
    "        train_lp_b_copy.drop(index,inplace=True)\n",
    "print(len(train_lp_b_copy))  \n",
    "train_lp_b = train_lp_b_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13118\n",
      "12963\n"
     ]
    }
   ],
   "source": [
    "dev_lp_b_copy = dev_lp_b\n",
    "print(len(dev_lp_b_copy))\n",
    "for index,row in dev_lp_b_copy.iterrows():\n",
    "    if len(row['t_answer'])<10:\n",
    "        dev_lp_b_copy.drop(index,inplace=True)\n",
    "print(len(dev_lp_b_copy))  \n",
    "dev_lp_b = dev_lp_b_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14992\n",
      "14808\n"
     ]
    }
   ],
   "source": [
    "test_lp_b_copy = test_lp_b\n",
    "print(len(test_lp_b_copy))\n",
    "for index,row in test_lp_b_copy.iterrows():\n",
    "    if len(row['t_answer'])<10:\n",
    "        test_lp_b_copy.drop(index,inplace=True)\n",
    "print(len(test_lp_b_copy))  \n",
    "test_lp_b = test_lp_b_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_a_b = np.stack(train_lp_b['t_answer'], axis=0)\n",
    "dev_a_b = np.stack(dev_lp_b['t_answer'], axis=0)\n",
    "train_y_b = np.stack(train_lp_b['cat_level'], axis=0)\n",
    "dev_y_b = np.stack(dev_lp_b['cat_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 100)           745000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10, 256)           25856     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10, 256)           65792     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 12805     \n",
      "=================================================================\n",
      "Total params: 930,253\n",
      "Trainable params: 185,053\n",
      "Non-trainable params: 745,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size_stop+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "bt = BatchNormalization()(answer_emb1)\n",
    "lstm = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(bt)\n",
    "\n",
    "dense1 = Dense(units=256, activation=\"relu\")(lstm)\n",
    "dense2 = Dense(units=256, activation=\"relu\")(dense1)\n",
    "\n",
    "flatten = Flatten()(dense2)\n",
    "\n",
    "out = Dense(5,  activation='softmax')(flatten)\n",
    "\n",
    "model = Model(inputs=[answer_inp], outputs=[out])\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98781 samples, validate on 19767 samples\n",
      "Epoch 1/30\n",
      "98781/98781 [==============================] - 20s 201us/step - loss: 1.4541 - accuracy: 0.3723 - val_loss: 1.3433 - val_accuracy: 0.4316\n",
      "Epoch 2/30\n",
      "98781/98781 [==============================] - 18s 182us/step - loss: 1.2717 - accuracy: 0.4693 - val_loss: 1.0919 - val_accuracy: 0.5693\n",
      "Epoch 3/30\n",
      "98781/98781 [==============================] - 19s 189us/step - loss: 1.1037 - accuracy: 0.5548 - val_loss: 0.9078 - val_accuracy: 0.6512\n",
      "Epoch 4/30\n",
      "98781/98781 [==============================] - 17s 177us/step - loss: 0.9968 - accuracy: 0.6030 - val_loss: 0.7884 - val_accuracy: 0.6960\n",
      "Epoch 5/30\n",
      "98781/98781 [==============================] - 18s 185us/step - loss: 0.9143 - accuracy: 0.6381 - val_loss: 0.7090 - val_accuracy: 0.7345\n",
      "Epoch 6/30\n",
      "98781/98781 [==============================] - 18s 186us/step - loss: 0.8538 - accuracy: 0.6662 - val_loss: 0.6143 - val_accuracy: 0.7778\n",
      "Epoch 7/30\n",
      "98781/98781 [==============================] - 19s 188us/step - loss: 0.8080 - accuracy: 0.6853 - val_loss: 0.5804 - val_accuracy: 0.7939\n",
      "Epoch 8/30\n",
      "98781/98781 [==============================] - 18s 183us/step - loss: 0.7655 - accuracy: 0.7039 - val_loss: 0.5379 - val_accuracy: 0.8060\n",
      "Epoch 9/30\n",
      "98781/98781 [==============================] - 17s 174us/step - loss: 0.7278 - accuracy: 0.7195 - val_loss: 0.5080 - val_accuracy: 0.8183\n",
      "Epoch 10/30\n",
      "98781/98781 [==============================] - 17s 174us/step - loss: 0.6958 - accuracy: 0.7333 - val_loss: 0.4572 - val_accuracy: 0.8380\n",
      "Epoch 11/30\n",
      "98781/98781 [==============================] - 17s 173us/step - loss: 0.6636 - accuracy: 0.7466 - val_loss: 0.4114 - val_accuracy: 0.8592\n",
      "Epoch 12/30\n",
      "98781/98781 [==============================] - 18s 185us/step - loss: 0.6352 - accuracy: 0.7585 - val_loss: 0.3840 - val_accuracy: 0.8692\n",
      "Epoch 13/30\n",
      "98781/98781 [==============================] - 19s 189us/step - loss: 0.6205 - accuracy: 0.7649 - val_loss: 0.3615 - val_accuracy: 0.8789\n",
      "Epoch 14/30\n",
      "98781/98781 [==============================] - 18s 181us/step - loss: 0.5923 - accuracy: 0.7757 - val_loss: 0.3517 - val_accuracy: 0.8780\n",
      "Epoch 15/30\n",
      "98781/98781 [==============================] - 18s 179us/step - loss: 0.5758 - accuracy: 0.7825 - val_loss: 0.3229 - val_accuracy: 0.8915\n",
      "Epoch 16/30\n",
      "98781/98781 [==============================] - 18s 184us/step - loss: 0.5540 - accuracy: 0.7899 - val_loss: 0.3108 - val_accuracy: 0.8974\n",
      "Epoch 17/30\n",
      "98781/98781 [==============================] - 18s 187us/step - loss: 0.5394 - accuracy: 0.7985 - val_loss: 0.2862 - val_accuracy: 0.9049\n",
      "Epoch 18/30\n",
      "98781/98781 [==============================] - 17s 174us/step - loss: 0.5230 - accuracy: 0.8016 - val_loss: 0.2697 - val_accuracy: 0.9135\n",
      "Epoch 19/30\n",
      "98781/98781 [==============================] - 17s 176us/step - loss: 0.5087 - accuracy: 0.8106 - val_loss: 0.2662 - val_accuracy: 0.9125\n",
      "Epoch 20/30\n",
      "98781/98781 [==============================] - 17s 173us/step - loss: 0.4886 - accuracy: 0.8185 - val_loss: 0.2479 - val_accuracy: 0.9180\n",
      "Epoch 21/30\n",
      "98781/98781 [==============================] - 18s 184us/step - loss: 0.4824 - accuracy: 0.8198 - val_loss: 0.2416 - val_accuracy: 0.9222\n",
      "Epoch 22/30\n",
      "98781/98781 [==============================] - 18s 180us/step - loss: 0.4674 - accuracy: 0.8265 - val_loss: 0.2311 - val_accuracy: 0.9273\n",
      "Epoch 23/30\n",
      "98781/98781 [==============================] - 19s 188us/step - loss: 0.4546 - accuracy: 0.8314 - val_loss: 0.2089 - val_accuracy: 0.9341\n",
      "Epoch 24/30\n",
      "98781/98781 [==============================] - 19s 190us/step - loss: 0.4439 - accuracy: 0.8346 - val_loss: 0.2023 - val_accuracy: 0.9355\n",
      "Epoch 25/30\n",
      "98781/98781 [==============================] - 17s 174us/step - loss: 0.4329 - accuracy: 0.8389 - val_loss: 0.1947 - val_accuracy: 0.9370\n",
      "Epoch 26/30\n",
      "98781/98781 [==============================] - 17s 174us/step - loss: 0.4243 - accuracy: 0.8432 - val_loss: 0.1918 - val_accuracy: 0.9393\n",
      "Epoch 27/30\n",
      "98781/98781 [==============================] - 18s 178us/step - loss: 0.4164 - accuracy: 0.8478 - val_loss: 0.1847 - val_accuracy: 0.9408\n",
      "Epoch 28/30\n",
      "98781/98781 [==============================] - 17s 175us/step - loss: 0.4060 - accuracy: 0.8506 - val_loss: 0.1778 - val_accuracy: 0.9417\n",
      "Epoch 29/30\n",
      "98781/98781 [==============================] - 18s 182us/step - loss: 0.3984 - accuracy: 0.8535 - val_loss: 0.1753 - val_accuracy: 0.9438\n",
      "Epoch 30/30\n",
      "98781/98781 [==============================] - 19s 189us/step - loss: 0.3910 - accuracy: 0.8557 - val_loss: 0.1614 - val_accuracy: 0.9480\n"
     ]
    }
   ],
   "source": [
    "model_glove_lstm_hist = model.fit(train_a, train_y, validation_data=(dev_a, dev_y), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64835 samples, validate on 12963 samples\n",
      "Epoch 1/30\n",
      "64835/64835 [==============================] - 12s 188us/step - loss: 0.3955 - accuracy: 0.8564 - val_loss: 0.0779 - val_accuracy: 0.9787\n",
      "Epoch 2/30\n",
      "64835/64835 [==============================] - 11s 177us/step - loss: 0.3508 - accuracy: 0.8714 - val_loss: 0.0765 - val_accuracy: 0.9793\n",
      "Epoch 3/30\n",
      "64835/64835 [==============================] - 11s 173us/step - loss: 0.3235 - accuracy: 0.8823 - val_loss: 0.0658 - val_accuracy: 0.9818\n",
      "Epoch 4/30\n",
      "64835/64835 [==============================] - 11s 176us/step - loss: 0.3071 - accuracy: 0.8883 - val_loss: 0.0615 - val_accuracy: 0.9845\n",
      "Epoch 5/30\n",
      "64835/64835 [==============================] - 11s 176us/step - loss: 0.3020 - accuracy: 0.8915 - val_loss: 0.0607 - val_accuracy: 0.9843\n",
      "Epoch 6/30\n",
      "64835/64835 [==============================] - 11s 177us/step - loss: 0.2879 - accuracy: 0.8962 - val_loss: 0.0631 - val_accuracy: 0.9832\n",
      "Epoch 7/30\n",
      "64835/64835 [==============================] - 11s 175us/step - loss: 0.2750 - accuracy: 0.9012 - val_loss: 0.0571 - val_accuracy: 0.9852\n",
      "Epoch 8/30\n",
      "64835/64835 [==============================] - 12s 184us/step - loss: 0.2726 - accuracy: 0.9037 - val_loss: 0.0531 - val_accuracy: 0.9867\n",
      "Epoch 9/30\n",
      "64835/64835 [==============================] - 12s 182us/step - loss: 0.2607 - accuracy: 0.9058 - val_loss: 0.0521 - val_accuracy: 0.9862\n",
      "Epoch 10/30\n",
      "64835/64835 [==============================] - 12s 178us/step - loss: 0.2588 - accuracy: 0.9069 - val_loss: 0.0538 - val_accuracy: 0.9849\n",
      "Epoch 11/30\n",
      "64835/64835 [==============================] - 12s 191us/step - loss: 0.2509 - accuracy: 0.9103 - val_loss: 0.0503 - val_accuracy: 0.9863\n",
      "Epoch 12/30\n",
      "64835/64835 [==============================] - 12s 184us/step - loss: 0.2473 - accuracy: 0.9119 - val_loss: 0.0470 - val_accuracy: 0.9876\n",
      "Epoch 13/30\n",
      "64835/64835 [==============================] - 12s 180us/step - loss: 0.2381 - accuracy: 0.9155 - val_loss: 0.0505 - val_accuracy: 0.9868\n",
      "Epoch 14/30\n",
      "64835/64835 [==============================] - 11s 174us/step - loss: 0.2320 - accuracy: 0.9181 - val_loss: 0.0502 - val_accuracy: 0.9860\n",
      "Epoch 15/30\n",
      "64835/64835 [==============================] - 11s 174us/step - loss: 0.2235 - accuracy: 0.9195 - val_loss: 0.0490 - val_accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "model_glove_lstm_hist_b = model.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = np.stack(test_lp['t_answer'], axis=0)\n",
    "test_y = np.stack(test_lp['cat_level'], axis=0)\n",
    "test_a_b = np.stack(test_lp_b['t_answer'], axis=0)\n",
    "test_y_b = np.stack(test_lp_b['cat_level'], axis=0)\n",
    "df_confusion = confusion_matrix(model, test_a_b, test_y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>none</th>\n",
       "      <th>mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>moderately severe</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>3520</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mild</th>\n",
       "      <td>7</td>\n",
       "      <td>2504</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderate</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3493</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderately severe</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>2745</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0              none  mild  moderate  moderately severe  severe\n",
       "row_0                                                             \n",
       "none               3520    14        10                  1       6\n",
       "mild                  7  2504        27                  6       3\n",
       "moderate             10     7      3493                  3       0\n",
       "moderately severe    19    11        25               2745      10\n",
       "severe               10     8        15                  4    2350"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.047430607787097456\n",
      "Test accuracy: 0.9867638945579529\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_a_b, test_y_b, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File got created\n"
     ]
    }
   ],
   "source": [
    "model.save('/home/nevronas/IG-GPUshare/Amit/models/model_glove_lstm_b.h5')\n",
    "json_dict = model_glove_lstm_hist.history\n",
    "with open('/home/nevronas/IG-GPUshare/Amit/models/model_glove_lstm_b_hist.json', 'w') as f:\n",
    "    f.write(str(json_dict))\n",
    "print('File got created')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
